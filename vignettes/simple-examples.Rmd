---
title: "Simple examples on how to use linear"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simple-examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Before we get started... load the package! 

```{r setup}
library(some.models)
```
#What is this package about? 

This package provides you with two simples functions to use in simple ways - **linear**, which allows you to run multiple linear regression via OLS, and **logistic**, which allows you to run a generalized linear model with the logit link function (meaning you are hopefully looking at binary outcomes). 

##linear 

What is OLS? Pretty much .. the easiest way to run a regression that we can think of. Essentially, it consists of estimating the parameters $\beta$ in order to minimize our loss in an $\ell-2$ sense. After a bit of calculus, we end up with a $\hat{\beta}_{p\times 1}$ estimate given by $(X^TX)^{-1}X^TY$ where $X$ is an $n \times p$, full-column rank design matrix of continuous outcomes.

### Does it work? 

Yes. 

```{r}
data(mtcars)
lm_fit_slr = lm(mpg ~ wt, mtcars)
cars_fit_slr = linear(mtcars$wt, mtcars$mpg)

summary(lm_fit_slr)
cars_fit_slr$coeffs
cars_fit_slr$se
cars_fit_slr$t

```

```{r}

lm_fit_mlr = lm(mpg ~ wt + hp, mtcars)
cars_fit_mlr = linear(cbind(mtcars$wt,mtcars$hp), mtcars$mpg)

summary(lm_fit_mlr)
cars_fit_mlr$coeffs
cars_fit_mlr$se
cars_fit_mlr$t

```

So all hypothesis testing (or at least hypothesis testing at the level of 650) can be done equivalently to the lm() function in base R. We get equivalent coefficient estimates, equivalent standard errors and equivalent t-statistics (as shown above). 
### How to use it?

* You can use it to (obviously) estimate slope and intercept coefficients. This leads to a way to visualize the regression (at least in the univariate case). 

```{r, fig.align = 'center', fig.width= 5, fig.height = 5}
library(ggplot2)

data(mtcars)

p = ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_bw() + xlab("Weight") + ylab("MPG") 
p1 = p + geom_abline(intercept = cars_fit_slr$coeffs[1], slope = cars_fit_slr$coeffs[2], color = 'red',size= 1.5)
p1 

```


This also checks out with the lm() function as well, but let's verify just in case. 

```{r, fig.align=  'center', fig.width= 5, fig.height = 5}
g = ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_bw() + xlab("Weight") + ylab("MPG") 
g1 = g + geom_smooth(method = lm, se = F)
g1
```

Same line!

* Something that is easy to do within the package is to test the basic assumptions of linearity. We covered linearity (above), so now we can cover constant variance by plotting the residuals versus the fitted values. 

```{r, fig.align=  'center', fig.width= 5, fig.height = 5}
var_data = as.data.frame(cbind(cars_fit_slr$res, cars_fit_slr$fitted_values))
colnames(var_data) = c('res','fits')

f = ggplot(var_data , aes(x = fits, y  = res)) + geom_point() + theme_bw() + ylab('Residuals') + xlab('Fitted Values')
f1 = f + geom_abline(intercept = 0.0, slope = 0, color = 'red', size = 1)
f1
```

```{r, fig.align = 'center', fig.width= 5, fig.height = 5}
q = ggplot(var_data, aes(sample = res)) + stat_qq() + theme_bw() 
q1 = q + ylab("Sample") + xlab('Theoretical') 
q1
```

* It is also easy to assess normality of the residuals using a QQ-plot. For further tests of collinear assumptions, it is best to consider your study design. If responses are measured over time, you can plot the residuals versus time and check to see if there is a trend (autocorrelation)
* The function also provides you with the opportunity to get predicted values from a set of data. 
```{r}
dim(mtcars)
train = sample(1:32, 16, replace = F)
mtcars_test = mtcars[-train,]
mtcars_train = mtcars[train,]

fit_predict = linear(cbind(mtcars_train$wt, mtcars_train$hp), mtcars_train$mpg, 
                     to_predict = cbind(mtcars_test$wt, mtcars_test$hp))

X_linear = fit_predict$predicted # much less work! 

fit_predict_lm = lm(mpg ~ wt + hp, data = mtcars_train)

X = cbind(rep(1,16), mtcars_test$wt, mtcars_test$hp)

X_lm = X %*% coefficients(fit_predict_lm)

all.equal(X_lm, X_linear)


```

### How does it compare in speed to lm( )? 

```{r, fig.align ='center', fig.width= 5, fig.height = 5}
library(bench)

X = matrix(sample(1:60,10000, replace = T),1000,10)
y = rnorm(1000,mean = 0 , sd = sqrt(10))

benchmark = bench::mark(as.vector(lm(y ~ X)$coefficients),as.vector(linear(X,y)$coeffs))

print(benchmark)
ggplot2::autoplot(benchmark)

```

It is faster than lm( ) on almost all runs. Furthemore, we can see it allocates less memory than lm( ), so it is better for performing very simple analysis. 

##logistic 

### Does it work? 

Also, yes. 

```{r}
admit_data = read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")

y = admit_data$admit

X =  admit_data$gre

glm_model = glm(admit ~ gre, data = admit_data, family = 'binomial')

logistic_fit = logistic(X,y)

all.equal(as.vector(logistic_fit$coeffs),as.vector(glm_model$coefficients))

all.equal(as.vector(logistic_fit$fitted),as.vector(glm_model$fitted.values))


```

The fitted values are slightly different, but very slightly and not enough to change predictive inference. We can also do prediction as we did before using logistic.

```{r}

dim(admit_data)
train = sample(1:400,200,replace = F)
admit_data_train = admit_data[train,]
admit_data_test = admit_data[-train,]

y = admit_data_train$admit

X = cbind(rep(1,dim(admit_data_train)[1]),
          admit_data_train$gre, 
          admit_data_train$gpa)

glm_model = glm(admit ~ gre + gpa, data = admit_data_train, family = 'binomial')

logistic_fit = logistic(X,y, to_predict = cbind(rep(1,dim(admit_data_test)[1]), 
                                                admit_data_test$gre, 
                                                admit_data_test$gpa), add_intercept = F)

head(logistic_fit$predicted,40)

```

This treats all vectors as row vectors, so say you have one covariate and 6 individuals you want to predict an outcome for. This prediction won't work and will throw an error if you pass them to the function with c(). You either need to add a column of ones, or to pass them individually and get individual predictions.  

This function will also work on data that is not just Bernoulli but binomial. Say we have a vector $Y$ of success and a vecotr $n$ of corresponding trials. So, an ordered pair could be something like (19,26) which means we observed 19 successes in 26 trials. This is possible in glm() - it is also possible with the logistic function. 

```{r}
data(mtcars)

library(dplyr)

mtcars$successes = sample(1:50, 32, replace = T)

mtcars$total = with(mtcars, successes + sample(1:15, 32, replace = T))

head(mtcars)

X = mtcars %>% select(wt,vs,am,successes,total)

glm_fit_binomial = glm(cbind(successes, total - successes) ~ wt + vs + am, data = X, family = 'binomial')
logistic_fit_binomial = logistic(cbind(rep(1,32), X$wt,X$vs,X$am), X$successes, n = X$total,add_intercept = F)

all.equal(as.vector(glm_fit_binomial$coefficients),as.vector(logistic_fit_binomial$coeffs))


```

So we see that we can use this logistic function to fit any data that uses the logit link function (so when the outcome follows Bernoulli or binomial)

### How does it compare to GLM? 

```{r message = F, , fig.align ='center', fig.width= 5, fig.height = 5}

X = matrix(sample(1:1000),100,10)
y = rbinom(100,1,0.6)
X1 = cbind(rep(1,100),X)

bmark = bench::mark(as.vector(glm(y ~ X, family = 'binomial')$coefficients), 
                    as.vector(logistic(X1,y,add_intercept = F)$coeffs))

as.vector(glm(y ~ X, family = 'binomial')$coefficients)

print(bmark)
ggplot2::autoplot(bmark)

```


So, in general, we are about 3 times slower. glm is written in C++, a faster language, but also does a lot more complex calcuations and returns lots more information. However, I have found that all the information provided by glm makes it *harder* to understand logistic regression (and GLMS in general). logistic gives you the basic information you need for starting off as a statistician - an important trait. 
